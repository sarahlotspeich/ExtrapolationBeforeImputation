# Plot our H(x) vs. R's transformed S(x)
data.frame(x = seq(0.01, 1, by = 0.01)) |>
mutate(HxOurs = Hx(x = x, alpha = alpha, lambda = lambda),
HxRs = -log(pgamma(q = x, shape = alpha, rate = lambda, lower.tail = FALSE))) |>
ggplot(aes(x = HxOurs, y = HxRs)) +
geom_point() +
geom_smooth() +
geom_abline(slope = 1, intercept = 0, linetype = 2, color = "red") +
theme_minimal() +
xlab(label = TeX(input = "Our $H(x)$")) +
ylab(label = TeX(input = "R's $H(x)$"))
hx_gamma = function(x, alpha = 2, lambda = 3, beta = 0) {
dgamma(x = x, shape = alpha, rate = lambda) / pgamma(q = x, shape = alpha, rate = lambda, lower.tail = FALSE) * exp(beta)
}
plot_hx = ggplot() +
stat_function(geom = "line", fun = hx_gamma, xlim = c(0, 5)) +
stat_function(geom = "line", fun = hx_gamma, xlim = c(0, 5), args = c(beta = 1)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
library(ggplot2)
library(latex2exp)
hx_gamma = function(x, alpha = 2, lambda = 3, beta = 0) {
dgamma(x = x, shape = alpha, rate = lambda) / pgamma(q = x, shape = alpha, rate = lambda, lower.tail = FALSE) * exp(beta)
}
plot_hx = ggplot() +
stat_function(geom = "line", fun = hx_gamma, xlim = c(0, 5)) +
stat_function(geom = "line", fun = hx_gamma, xlim = c(0, 5), args = c(beta = 1)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
plot_hx
ggplot() +
stat_function(geom = "line", fun = hx_gamma, xlim = c(0, 5)) +
stat_function(geom = "line", fun = hx_gamma, xlim = c(0, 5), args = c(beta = 1)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
plot_hx = ggplot() +
stat_function(geom = "line", fun = hx_gamma, xlim = c(1E-4, 5)) +
stat_function(geom = "line", fun = hx_gamma, xlim = c(1E-4, 5), args = c(beta = 1)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
plot_hx
hx = function(x, alpha = 2, lambda = 3, beta = 0, z = 0) {
dweibull(x = x, shape = alpha, scale = lambda) / pweibull(q = x, shape = alpha, scale = lambda, lower.tail = FALSE) * exp(beta * z)
}
data.frame(x = seq(0, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))),
beta = 1) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = haz, col = z)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
data.frame(x = seq(0, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))),
beta = 1) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = factor(z))) |>
ggplot() +
geom_line(aes(x = x, y = haz, col = z)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
data.frame(x = seq(0, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))),
beta = 1) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = haz, col = factor(z))) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
data.frame(x = seq(0, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))),
beta = 1) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = factor(z))) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
beta = rep(c(-1, 1, 2), each = length(rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))))) |>
data.frame(x = seq(0, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))),
beta = rep(c(-1, 1, 2),
each = length(rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = factor(z))) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$"))
data.frame(x = seq(0, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))),
beta = rep(c(-1, 1, 2),
each = length(rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = factor(z))) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta)
data.frame(x = seq(0, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(0, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = factor(z))) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta)
data.frame(x = seq(1E-4, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = factor(z))) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta) +
theme(legend.position = "top")
data.frame(x = seq(1E-2, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(1E-2, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z)) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = factor(z))) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta) +
theme(legend.position = "top")
data.frame(x = seq(1E-2, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(1E-2, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z),
z = factor(z, labels = c("Z = 0", "Z = 1", "Z = 2"))) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = z)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta) +
theme(legend.position = "top")
data.frame(x = seq(1E-2, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(1E-2, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z),
Covariate = factor(z, labels = c("Z = 0", "Z = 1", "Z = 2"))) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = Covariate)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta) +
theme(legend.position = "top")
data.frame(x = seq(1E-2, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(1E-2, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z),
Covariate = factor(z, labels = c("Z = 0", "Z = 1", "Z = 2")),
beta = factor(beta,
labels = c(TeX("$\\beta = -2$"), TeX("$\\beta = -1$"),
TeX("$\\beta = 0$"),
TeX("$\\beta = 1$"), TeX("$\\beta = 2$")
))) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = Covariate)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta) +
theme(legend.position = "top", labeller = label_parsed, nrow = 1)
data.frame(x = seq(1E-2, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(1E-2, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z),
Covariate = factor(z, labels = c("Z = 0", "Z = 1", "Z = 2")),
beta = factor(beta,
labels = c(TeX("$\\beta = -2$"), TeX("$\\beta = -1$"),
TeX("$\\beta = 0$"),
TeX("$\\beta = 1$"), TeX("$\\beta = 2$")
))) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = Covariate)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$h(x)$")) +
facet_wrap(~beta, labeller = label_parsed, nrow = 1) +
theme(legend.position = "top")
ggsave(filename = "~/Downloads/prop-haz.png", height = 6, width = 10, units = "in")
data.frame(x = seq(1E-2, 5, by = 0.01),
z = rep(c(0, 1, 2), each = length(seq(1E-4, 5, by = 0.01))),
beta = rep(seq(-2, 2),
each = length(rep(c(0, 1, 2), each = length(seq(1E-2, 5, by = 0.01)))))) |>
dplyr::mutate(haz = hx(x = x, beta = beta, z = z),
Covariate = factor(z, labels = c("Z = 0", "Z = 1", "Z = 2")),
beta = factor(beta,
labels = c(TeX("$\\beta = -2$"), TeX("$\\beta = -1$"),
TeX("$\\beta = 0$"),
TeX("$\\beta = 1$"), TeX("$\\beta = 2$")
))) |>
ggplot() +
geom_line(aes(x = x, y = log(haz), col = Covariate)) +
theme_minimal() +
xlab(label = TeX(input = "$x$")) +
ylab(label = TeX(input = "$\\log\\{h(x)\\}$")) +
facet_wrap(~beta, labeller = label_parsed, nrow = 1) +
theme(legend.position = "top")
ggsave(filename = "~/Downloads/prop-haz.png", height = 6, width = 10, units = "in")
# //////////////////////////////////////////////////////////////////////
# Replicate Table 1 ////////////////////////////////////////////////////
# Caption begins "Simulation results for Weibull $X$ from the full /////
# cohort analysis and imputation approaches using the true survival ////
# function and adaptive quadrature versus the trapezoidal rule..." /////
# //////////////////////////////////////////////////////////////////////
# Load packages
library(dplyr) # For data wrangling
library(tidyr) # To gather wide tables
library(kableExtra) # To format pretty tables
# //////////////////////////////////////////////////////////////////////
# Read in simulation results from GitHub ///////////////////////////////
# //////////////////////////////////////////////////////////////////////
# Read in simulation results
#res = read.csv(file = "https://raw.githubusercontent.com/sarahlotspeich/ItsIntegral/main/Table-Data/data_Table1.csv")
res = read.csv(file = "https://raw.githubusercontent.com/sarahlotspeich/ItsIntegral/main/Table-Data/data_Table1_XdepZ.csv")
## Note: Simulations were run in parallel on random seeds 114-123 (with 100 reps per seed, per setting)
## This information is captured in the "sim" variable which is of the form seed-replicate.
# Calculate average % censoring per censoring setting
res |>
group_by(censoring) |>
summarize(avg_perc_censored = mean(perc_censored))
# //////////////////////////////////////////////////////////////////////
# Summarize simulation results by setting //////////////////////////////
# //////////////////////////////////////////////////////////////////////
# Unpivot results to get rows for each parameter (rather than columns)
res_summ_long = res |>
dplyr::select(-perc_censored) |> # use package prefix to avoid conflict with MASS::select
gather(key = "param_calc", value = "est", -c(1:3)) |>
mutate(calc = gsub(pattern = ".*_", replacement = "", x = param_calc),
calc = factor(x = calc,
levels = c("fc", "aq", "tr"),
labels = c("Full Cohort", "Adaptive Quadrature", "Trapezoidal Rule")),
param = sub("_.*", "", param_calc),
censoring = factor(x = censoring,
levels = c("light", "heavy", "extra_heavy"),
labels = c("Light", "Heavy", "Extra Heavy")),
truth = ifelse(test = param == "alpha",
yes = 1,
no = ifelse(test = param == "beta",
yes = 0.5,
no = 0.25)
)
) |>
group_by(censoring, n, calc, param, truth) |>
summarize(bias = mean(est - truth),
se = sd(est)) |>
mutate(perc_bias = paste0("($", format(round(bias / truth * 100, 2), nsmall = 2), "$)"),
bias = paste0("$", format(round(bias, 3), nsmall = 3), "$")
) |>
ungroup()
# Then pivot them back out by method
res_summ_wide = res_summ_long |>
pivot_wider(names_from = calc,
values_from = c("bias", "se")) |>
arrange(param, censoring) |>
mutate(`re_Adaptive Quadrature` = `se_Full Cohort` ^ 2 / `se_Adaptive Quadrature` ^ 2,
`re_Trapezoidal Rule` = `se_Full Cohort` ^ 2 / `se_Trapezoidal Rule` ^ 2,
mid1 = "", mid2 = "", mid3 = "") |>
dplyr::select(censoring, n, param, mid1,
ends_with("Cohort"), mid2,
ends_with("Quadrature"), mid3,
ends_with("Rule"))
# //////////////////////////////////////////////////////////////////////
# Format table for export to LaTex /////////////////////////////////////
# //////////////////////////////////////////////////////////////////////
# Write function to add "padded" zeros and wrap with $$ for consistency
format_num = function(num) {
paste0("$", format(round(num, 3), nsmall = 3), "$")
}
# Format res_summ_wide for LaTex
res_summ_wide |>
dplyr::select(-param) |> # delete param column - ordering is alpha, beta, gamma
mutate_at(.vars = c("se_Full Cohort",
"se_Adaptive Quadrature", "re_Adaptive Quadrature",
"se_Trapezoidal Rule", "re_Trapezoidal Rule"), .funs = format_num) |>
kable(format = "latex", booktabs = TRUE, escape = FALSE,
align = "llrcccccccrccccccc") |>
kable_styling()
## Note: For visual reasons, the \addlinespace were manually deleted in LaTex
## And a \multicolumn used to separate the three parameters
head(res_summ_long)
setwd("~/Dropbox (Wake Forest University)/0 - CODE/ItsIntegral/Table-Data/Table1_XdepZ")
# //////////////////////////////////////////////////////////////////////
# Run simulation results for Table 1 ///////////////////////////////////
# Compare full cohort analysis to "gold standard" CMI based on true ////
# function and adaptive quadrature vs. trapezoidal rule for Weibull X //
# //////////////////////////////////////////////////////////////////////
# Load packages
## Run once: install.packages("devtools")
## Run once: devtools::install_github("sarahlotspeich/imputeCensRd)
library(imputeCensRd) # To impute censored covariates
# Data generation function based on Atem et al. (2017)'s "independent censoring" censoring
generate_AtemSMMR = function(n, censoring = "light") {
z = rbinom(n = n, size = 1, prob = 0.5) # Uncensored covariate
x = rweibull(n = n, shape = 0.75 - 0.25 * z, scale = 0.25)  # To-be-censored covariate
e = rnorm(n = n, mean = 0, sd = 1) # Random errors
y = 1 + 0.5 * x + 0.25 * z + e # Continuous outcome
q = ifelse(test = censoring == "light",
yes = 0.5, ## ~ 12%
no = ifelse(test = censoring == "heavy",
yes = 2.9, ## ~ 41%
no = 20) ## ~ 78%
) # Rate parameter for censoring
c = rexp(n = n, rate = q) # Random censoring mechanism
w = pmin(x, c) # Observed covariate value
d = as.numeric(x <= c) # "Event" indicator
dat = data.frame(x, z, w, y, d) # Construct data set
return(dat)
}
# Write a function for the true survival function used to generate Weibull X
trueSURV = function(q, z) {
pweibull(q = q, shape = 0.75 - 0.25 * z, scale = 0.25, lower.tail = FALSE)
}
# Set the number of replicates per setting
reps = 1000
# Choose seed
sim_seed = 114
# Loop over different censoring rates: light, heavy, extra heavy
for (censoring in c("light", "heavy", "extra_heavy")) {
# And different sample sizes n = 100, 500, 1000, 2000
for (n in c(100, 500, 1000, 2000)){
# For reproducibility
set.seed(sim_seed)
# Create dataframe to save results for setting
sett_res = data.frame(sim = paste(sim_seed, 1:reps, sep = "-"), censoring, n, perc_censored = NA,
alpha_fc = NA, beta_fc = NA, gamma_fc = NA,
alpha_aq = NA, beta_aq = NA, gamma_aq = NA,
alpha_tr = NA, beta_tr = NA, gamma_tr = NA
)
# Loop over replicates
for (r in 1:reps) {
# Generate data
dat = generate_AtemSMMR(n = n,
censoring = censoring)
# Save % censored
sett_res$perc_censored[r] = 1 - mean(dat$d)
# Method 1: Full cohort analysis
fit_fc = lm(y ~ x + z, data = dat)
sett_res[r, c("alpha_fc", "beta_fc", "gamma_fc")] <- fit_fc$coefficients
# Method 2: CMI using adaptive quadrature
## Create imputed dataset
imp_dat = cmi_custom(W = "w", Delta = "d", Z = "z", data = dat,
useSURV = trueSURV, trapezoidal_rule = FALSE)
## Check that imputation was successful
## (it always is when using trueSURV)
if (imp_dat$code) {
## Fit model to imputed dataset
fit_aq = lm(y ~ imp + z, data = imp_dat$imputed_data)
sett_res[r, c("alpha_aq", "beta_aq", "gamma_aq")] <- fit_aq$coefficients
}
# Method 3: CMI using trapezoidal rule
imp_dat = cmi_custom(W = "w", Delta = "d", Z = "z", data = dat,
useSURV = trueSURV, trapezoidal_rule = TRUE)
## Check that imputation was successful
## (it always is when using trueSURV)
if (imp_dat$code) {
## Fit model to imputed dataset
fit_tr = lm(y ~ imp + z, data = imp_dat$imputed_data)
sett_res[r, c("alpha_tr", "beta_tr", "gamma_tr")] <- fit_tr$coefficients
}
# Save results
write.csv(x = sett_res,
file = paste0(censoring, "_n", n, "_seed", sim_seed, ".csv"),
row.names = F)
}
}
}
# //////////////////////////////////////////////////////////////////////
# NOTES: When using the true survival function, there is no need  //////
# to fit an imputation model or use an extrapolation method. This //////
# makes the simulations for this table quicker than other tables. //////
# It took <1 second to run 1 replication per setting MacBook Pro (M1) //
# with 16GB RAM. Based on this, it would take ~11 minutes to run 1000 //
# replications per setting. ////////////////////////////////////////////
# //////////////////////////////////////////////////////////////////////
setwd("~/Dropbox (Wake Forest University)/0 - CODE/ItsIntegral/Table-Data/Table1_XdepZ/")
f = list.files()
d = do.call(dplyr::bind_rows,
lapply(X = f, FUN = read.csv))
d |>
write.csv("~/Dropbox (Wake Forest University)/0 - CODE/ItsIntegral/Table-Data/data_Table1_XdepZ.csv",
row.names = F)
# //////////////////////////////////////////////////////////////////////
# Replicate Table 1 ////////////////////////////////////////////////////
# Caption begins "Simulation results for Weibull $X$ from the full /////
# cohort analysis and imputation approaches using the true survival ////
# function and adaptive quadrature versus the trapezoidal rule..." /////
# //////////////////////////////////////////////////////////////////////
# Load packages
library(dplyr) # For data wrangling
library(tidyr) # To gather wide tables
library(kableExtra) # To format pretty tables
# //////////////////////////////////////////////////////////////////////
# Read in simulation results from GitHub ///////////////////////////////
# //////////////////////////////////////////////////////////////////////
# Read in simulation results
#res = read.csv(file = "https://raw.githubusercontent.com/sarahlotspeich/ItsIntegral/main/Table-Data/data_Table1.csv")
res = read.csv(file = "https://raw.githubusercontent.com/sarahlotspeich/ItsIntegral/main/Table-Data/data_Table1_XdepZ.csv")
## Note: Simulations were run in parallel on random seeds 114-123 (with 100 reps per seed, per setting)
## This information is captured in the "sim" variable which is of the form seed-replicate.
# Calculate average % censoring per censoring setting
res |>
group_by(censoring) |>
summarize(avg_perc_censored = mean(perc_censored))
# //////////////////////////////////////////////////////////////////////
# Summarize simulation results by setting //////////////////////////////
# //////////////////////////////////////////////////////////////////////
# Unpivot results to get rows for each parameter (rather than columns)
res_summ_long = res |>
dplyr::select(-perc_censored) |> # use package prefix to avoid conflict with MASS::select
gather(key = "param_calc", value = "est", -c(1:3)) |>
mutate(calc = gsub(pattern = ".*_", replacement = "", x = param_calc),
calc = factor(x = calc,
levels = c("fc", "aq", "tr"),
labels = c("Full Cohort", "Adaptive Quadrature", "Trapezoidal Rule")),
param = sub("_.*", "", param_calc),
censoring = factor(x = censoring,
levels = c("light", "heavy", "extra_heavy"),
labels = c("Light", "Heavy", "Extra Heavy")),
truth = ifelse(test = param == "alpha",
yes = 1,
no = ifelse(test = param == "beta",
yes = 0.5,
no = 0.25)
)
) |>
group_by(censoring, n, calc, param, truth) |>
summarize(bias = mean(est - truth),
se = sd(est)) |>
mutate(perc_bias = paste0("($", format(round(bias / truth * 100, 2), nsmall = 2), "$)"),
bias = paste0("$", format(round(bias, 3), nsmall = 3), "$")
) |>
ungroup() |>
select(param, censoring, n, calc, bias, perc_bias, se)
head(res_summ_long)
# //////////////////////////////////////////////////////////////////////
# Replicate Table 1 ////////////////////////////////////////////////////
# Caption begins "Simulation results for Weibull $X$ from the full /////
# cohort analysis and imputation approaches using the true survival ////
# function and adaptive quadrature versus the trapezoidal rule..." /////
# //////////////////////////////////////////////////////////////////////
# Load packages
library(dplyr) # For data wrangling
library(tidyr) # To gather wide tables
library(kableExtra) # To format pretty tables
# //////////////////////////////////////////////////////////////////////
# Read in simulation results from GitHub ///////////////////////////////
# //////////////////////////////////////////////////////////////////////
# Read in simulation results
#res = read.csv(file = "https://raw.githubusercontent.com/sarahlotspeich/ItsIntegral/main/Table-Data/data_Table1.csv")
res = read.csv(file = "https://raw.githubusercontent.com/sarahlotspeich/ItsIntegral/main/Table-Data/data_Table1_XdepZ.csv")
## Note: Simulations were run in parallel on random seeds 114-123 (with 100 reps per seed, per setting)
## This information is captured in the "sim" variable which is of the form seed-replicate.
# Calculate average % censoring per censoring setting
res |>
group_by(censoring) |>
summarize(avg_perc_censored = mean(perc_censored))
# //////////////////////////////////////////////////////////////////////
# Summarize simulation results by setting //////////////////////////////
# //////////////////////////////////////////////////////////////////////
# Unpivot results to get rows for each parameter (rather than columns)
res_summ_long = res |>
dplyr::select(-perc_censored) |> # use package prefix to avoid conflict with MASS::select
gather(key = "param_calc", value = "est", -c(1:3)) |>
mutate(calc = gsub(pattern = ".*_", replacement = "", x = param_calc),
calc = factor(x = calc,
levels = c("fc", "aq", "tr"),
labels = c("Full Cohort", "Adaptive Quadrature", "Trapezoidal Rule")),
param = sub("_.*", "", param_calc),
censoring = factor(x = censoring,
levels = c("light", "heavy", "extra_heavy"),
labels = c("Light", "Heavy", "Extra Heavy")),
truth = ifelse(test = param == "alpha",
yes = 1,
no = ifelse(test = param == "beta",
yes = 0.5,
no = 0.25)
)
) |>
group_by(censoring, n, calc, param, truth) |>
summarize(bias = mean(est - truth),
se = sd(est)) |>
mutate(perc_bias = paste0("($", format(round(bias / truth * 100, 2), nsmall = 2), "$)"),
bias = paste0("$", format(round(bias, 3), nsmall = 3), "$")
) |>
ungroup() |>
select(param, censoring, n, calc, bias, perc_bias, se)
head(res_summ_long)
